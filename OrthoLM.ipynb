{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87a2b177-8d56-49dd-9def-33b01de7cf25",
   "metadata": {},
   "source": [
    "# Loading packages and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "821d1131-9dae-4322-b2b0-23a8d09b9a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thomas/.local/lib/python3.10/site-packages/Bio/Application/__init__.py:40: BiopythonDeprecationWarning: The Bio.Application modules and modules relying on it have been deprecated.\n",
      "\n",
      "Due to the on going maintenance burden of keeping command line application\n",
      "wrappers up to date, we have decided to deprecate and eventually remove these\n",
      "modules.\n",
      "\n",
      "We instead now recommend building your command line and invoking it directly\n",
      "with the subprocess module.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from matplotlib_venn import venn2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import time\n",
    "import csv\n",
    "import json # for saving dictionnaries\n",
    "import pickle # for saving dictionnaries\n",
    "import copy\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import esm\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import adjusted_mutual_info_score\n",
    "from collections import Counter, OrderedDict\n",
    "from Bio.Blast import Applications\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79db364c-65e0-4a65-a5db-15a464fce487",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "stack expects a non-empty TensorList",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m         prot_names_and_group\u001b[38;5;241m.\u001b[39mappend(header\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m|\u001b[39m\u001b[38;5;124m'\u001b[39m))        \n\u001b[1;32m     25\u001b[0m         aaSequences\u001b[38;5;241m.\u001b[39mappend(_seq)\n\u001b[0;32m---> 27\u001b[0m Xs \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(Xs\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     30\u001b[0m Xs_train \u001b[38;5;241m=\u001b[39m Xs\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects a non-empty TensorList"
     ]
    }
   ],
   "source": [
    "# MODIFICATION OF THE PATHS\n",
    "path_to_data = \"Datasets/\"\n",
    "species = \"pfal_pber\"\n",
    "FASTA_PATH = path_to_data + species + \".fasta\"\n",
    "\n",
    "model_list = [\"esm2_t48_15B_UR50D\", \"esm2_t36_3B_UR50D\", \"esm2_t33_650M_UR50D\", \"esm2_t30_150M_UR50D\", \"esm2_t12_35M_UR50D\", \"esm2_t6_8M_UR50D\"]\n",
    "EMB_LAYER_list = [48, 36, 33, 30, 12, 6]\n",
    "\n",
    "model_index = 3 # change to select the model\n",
    "model = model_list[model_index]\n",
    "EMB_LAYER = EMB_LAYER_list[model_index]\n",
    "\n",
    "EMB_PATH = path_to_data + species + \"_emb_\" + model + \"/\"\n",
    "\n",
    "\n",
    "Xs = []\n",
    "prot_names_and_group = []\n",
    "aaSequences = []\n",
    "for header, _seq in esm.data.read_fasta(FASTA_PATH):\n",
    "    fn = f'{EMB_PATH}/{header[0:]}.pt'\n",
    "    if os.path.isfile(fn):\n",
    "        embs = torch.load(fn)\n",
    "        Xs.append(embs['mean_representations'][EMB_LAYER])\n",
    "        prot_names_and_group.append(header.split('|'))        \n",
    "        aaSequences.append(_seq)\n",
    "\n",
    "Xs = torch.stack(Xs, dim=0).numpy()\n",
    "print(Xs.shape)\n",
    "\n",
    "Xs_train = Xs\n",
    "prot_names_and_group_train = prot_names_and_group"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540d7f56-acf8-4c26-ae50-64292068d1d1",
   "metadata": {},
   "source": [
    "This code handles proteome fasta files downloaded from OrthoMCL-DB.\n",
    "\n",
    "In particular the variable prot_names_and_group stores protein information under the format: >species_code|protein_code | organism=Species full name | Protein description | Ortholog_group_id\n",
    "\n",
    "For other databases, adapt accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4f39f6-a917-49a3-aec9-80f7bef2c4aa",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "208be962-432c-4fa6-b564-3bb9a8d83fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_num_pca_components = [20, 40, 60, 80, 100, 120, 0]\n",
    "list_num_pca_components = [20]\n",
    "list_n_clusters = [int(Xs_train.shape[0] / 2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329a33cd-fd7e-4406-8196-e00fe7d7e0b5",
   "metadata": {},
   "source": [
    "### New run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2d8a830f-ebb8-4de7-ba33-975d7bb43034",
   "metadata": {},
   "outputs": [],
   "source": [
    "def saving_from_kmeans(Xs_train_pca, prot_names_and_group_train, kmeans):\n",
    "    \n",
    "    n_clusters = kmeans.n_clusters\n",
    "    n_samples = Xs_train_pca.shape[0]\n",
    "    X_labels = kmeans.labels_\n",
    "    # We do not save training data Xs_train_pca because of memory requirements. It can be recreated quickly if necessary.\n",
    "\n",
    "    return [n_clusters, n_samples, prot_names_and_group_train, X_labels, kmeans,\n",
    "            \"n_clusters, n_samples, prot_names_and_group_train, X_labels, kmeans\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2f09b1ff-9e1f-4cf7-af8f-11d4ac451f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_pca_components: 20\n",
      "n_clusters: 5131\n",
      "Elapsed time: 22.357588529586792 seconds\n",
      "Saving results\n"
     ]
    }
   ],
   "source": [
    "kmeans_saving = {}\n",
    "for num_pca_components in list_num_pca_components:\n",
    "    print(\"num_pca_components: \" + str(num_pca_components))\n",
    "    t0 = time.time()\n",
    "    if num_pca_components==0:\n",
    "        Xs_train_pca = Xs_train\n",
    "    else:\n",
    "        pca = PCA(n_components = num_pca_components, svd_solver = \"full\")\n",
    "        Xs_train_pca = pca.fit_transform(Xs_train)\n",
    "\n",
    "    kmeans_saving[\"n_pca\" + str(num_pca_components)] = {}\n",
    "    for n_clusters in list_n_clusters:\n",
    "        print(\"n_clusters: \" + str(n_clusters))\n",
    "        kmeans = KMeans(n_clusters = n_clusters, n_init=5, random_state=0).fit(Xs_train_pca)\n",
    "        kmeans_saving[\"n_pca\" + str(num_pca_components)][\"n_clusters\" + str(n_clusters)] = saving_from_kmeans(Xs_train_pca, prot_names_and_group_train, kmeans)\n",
    "        \n",
    "    print(\"Elapsed time: \" + str(time.time() - t0) + \" seconds\")\n",
    "    kmeans_saving[\"n_pca\" + str(num_pca_components)][\"running_time_by_PCA\"] = str(time.time() - t0) + \" seconds\"\n",
    "    kmeans_saving[\"n_pca\" + str(num_pca_components)][\"Xs_train_pca\"] = Xs_train_pca\n",
    "\n",
    "if True: # Change to True for saving the results\n",
    "    print(\"Saving results\")\n",
    "    with open(path_to_data + \"Results/\" + species + \"_\" + model + \"_\" + str(len(list_num_pca_components)) + \"PCA_\" +\n",
    "              str(len(list_n_clusters)) + \"cluster_kmeans_save.pkl\", 'wb') as fp:\n",
    "        pickle.dump(kmeans_saving, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ab726f-4149-468c-a649-6c51b77d3c6d",
   "metadata": {},
   "source": [
    "### Loading results from a previous run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3b2e3284-556e-46ee-9f1f-d4c17cd5f59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_to_data + \"Results/pfal_pber_esm2_t33_650M_UR50D_1PCA_1cluster_kmeans_save.pkl\", 'rb') as f:\n",
    "    kmeans_saving = pickle.load(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "89c37a4f-8c0d-41ba-83b9-8752aebb9939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['n_pca20']\n",
      "['n_clusters5131', 'running_time_by_PCA', 'Xs_train_pca']\n"
     ]
    }
   ],
   "source": [
    "print(list(kmeans_saving.keys()))\n",
    "print(list(kmeans_saving[list(kmeans_saving.keys())[0]].keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9d00c0-f021-41d7-8533-720f399fe204",
   "metadata": {},
   "source": [
    "# Analysing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "723a2305-a788-4a21-9c42-1b350b50ff05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_pairwise_performance(saved_results, Xs_train_pca):\n",
    "\n",
    "    n_clusters = saved_results[0]\n",
    "    n_samples = saved_results[1]\n",
    "    prot_names_and_group_train = saved_results[2]\n",
    "    X_labels = saved_results[3]\n",
    "    kmeans = saved_results[4]\n",
    "\n",
    "    X_dist = kmeans.transform(Xs_train_pca)**2 # distance from each point to all of the clusters\n",
    "    \n",
    "    orthologs_naiveSearch = []\n",
    "    orthologs_distanceBasedSearch = []\n",
    "    orthologs_1_to_1 = []\n",
    "    orthologs_SATURN = []\n",
    "    \n",
    "    n_species_total = len(list(set([prot[0] for prot in prot_names_and_group_train])))\n",
    "    for cluster in range(n_clusters):\n",
    "        ind_fromCluster = [i for i, x in enumerate(X_labels) if x==cluster] # get all indices of sequences in that cluster\n",
    "        if len(ind_fromCluster) == 1:\n",
    "            continue # skipping an iteration in case finding a cluster of size 1\n",
    "\n",
    "        ind_sorted = np.argsort(X_dist[ind_fromCluster,cluster]) # sort by increasing distance to the centroid\n",
    "        all_prots = [prot_names_and_group_train[ind_fromCluster[i]] for i in ind_sorted]\n",
    "        all_specs = [prot_names_and_group_train[ind_fromCluster[i]][0] for i in ind_sorted]\n",
    "        \n",
    "        # Naive search for ortholog combinations\n",
    "        if len(list(set(all_specs))) > 1: # need at least 2 different species inside the cluster\n",
    "            for i1 in range(len(all_specs) - 1): # we explore all pairwise combinations that give orthologs\n",
    "                for i2 in range(i1 + 1, len(all_specs)):\n",
    "                    if all_specs[i1] != all_specs[i2]:\n",
    "                        orthologs_naiveSearch.append([all_prots[i1], all_prots[i2]])\n",
    "        \n",
    "        # Distance-based search for ortholog among the 2 sequences closest to the centroid\n",
    "        all_prots_1st_pair = all_prots[0:2]\n",
    "        if all_prots_1st_pair[0][0] != all_prots_1st_pair[1][0]: # search for orthologs (different species)\n",
    "            orthologs_distanceBasedSearch.append(all_prots_1st_pair)\n",
    "\n",
    "        # Search for 1 to 1 orthologs\n",
    "        if len(all_specs) == len(list(set(all_specs))) == n_species_total:\n",
    "            orthologs_1_to_1.append(all_prots)\n",
    "\n",
    "        # SATURN search (build pairs by increasing distances from the centroid)\n",
    "        ind_species2 = 1\n",
    "        if all_specs[0] != all_specs[ind_species2]:\n",
    "            orthologs_SATURN.append([all_prots[0], all_prots[ind_species2]])\n",
    "        else:\n",
    "            while all_specs[0] == all_specs[ind_species2]:\n",
    "                ind_species2 += 1\n",
    "                if ind_species2 == len(all_specs):\n",
    "                    break\n",
    "                if all_specs[0] != all_specs[ind_species2]:\n",
    "                    orthologs_SATURN.append([all_prots[0], all_prots[ind_species2]])\n",
    "                    break\n",
    "   \n",
    "    return [orthologs_naiveSearch,\n",
    "            orthologs_distanceBasedSearch,\n",
    "            orthologs_1_to_1,\n",
    "            orthologs_SATURN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c7746554-1aa4-4f5d-b793-08d565df856f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def measure_group_performance(saved_results):\n",
    "\n",
    "    n_clusters = saved_results[0]\n",
    "    n_samples = saved_results[1]\n",
    "    prot_names_and_group_train = saved_results[2]\n",
    "    X_labels = saved_results[3]\n",
    "    kmeans = saved_results[4]\n",
    "\n",
    "    toReturn = []\n",
    "\n",
    "    # First, make a list of all groups present in each cluster.\n",
    "    list_groups_in_clusters = [[] for i in range(n_clusters)]\n",
    "    for i_label in range(n_samples):\n",
    "        list_groups_in_clusters[X_labels[i_label]].append(prot_names_and_group_train[i_label][4])\n",
    "    #print(list_groups_in_clusters)\n",
    "\n",
    "    # Gather all true group names\n",
    "    list_all_groups = list(set([prot[4] for prot in prot_names_and_group_train])) # remove duplicated names\n",
    "    \n",
    "    # Count how many sequences from each group are present in each cluster\n",
    "    list_OG_count_in_cluster = np.zeros((len(list_all_groups), n_clusters))\n",
    "    for i_cluster in range(n_clusters):\n",
    "        for group in list_groups_in_clusters[i_cluster]:\n",
    "            list_OG_count_in_cluster[list_all_groups.index(group), i_cluster] += 1\n",
    "    if not(np.sum(list_OG_count_in_cluster) == n_samples):\n",
    "        print(\"Error: missing some sequences in the count matrix\")\n",
    "    #print(list_OG_count_in_cluster)\n",
    "\n",
    "    family_complet_stat = 0\n",
    "    for i_group in range(len(list_all_groups)):\n",
    "        family_complet_stat += max(list_OG_count_in_cluster[i_group,:])\n",
    "    family_complet_stat = family_complet_stat / n_samples\n",
    "    toReturn.append(family_complet_stat)\n",
    "\n",
    "    toReturn.append(adjusted_mutual_info_score([prot[4] for prot in prot_names_and_group_train], X_labels))\n",
    "\n",
    "    # Count groups that are totally and exclusively contained in a single cluster\n",
    "    i_count_success_exactMatch = 0\n",
    "    for i_group in range(len(list_all_groups)):\n",
    "        if np.count_nonzero(list_OG_count_in_cluster[i_group,:] == 0) == n_clusters - 1: # check if all sequences from the OG belong to only one of the 100 cluster, and the other clusters contain 0 sequences from that group\n",
    "            # Check if the corresponding cluster contains only that group\n",
    "            i_cluster = np.nonzero(list_OG_count_in_cluster[i_group,:])[0][0]\n",
    "            if np.count_nonzero(list_OG_count_in_cluster[:,i_cluster] == 0) == len(list_all_groups) - 1:\n",
    "                i_count_success_exactMatch += 1\n",
    "\n",
    "    toReturn.append(i_count_success_exactMatch / n_clusters)\n",
    "\n",
    "    toReturn.append(\"Family Completeness, Adjusted Mutual Information, Exact matches over total\")\n",
    "    return(toReturn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1465998d-9815-4fed-9117-aa9de3802849",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_total_count_n2m_orthologs(prot_names_and_group_train):\n",
    "    list_all_OGs = list(set([protein[4] for protein in prot_names_and_group_train]))\n",
    "    \n",
    "    list_proteins_in_OG = [[] for i in range(len(list_all_OGs))]\n",
    "    \n",
    "    for protein in prot_names_and_group_train:\n",
    "        list_proteins_in_OG[list_all_OGs.index(protein[4])].append(protein[0])\n",
    "\n",
    "    names_species = list(set([protein[0] for protein in prot_names_and_group_train]))\n",
    "    \n",
    "    total_n2m_ortholog_count = 0\n",
    "    for list_proteins in list_proteins_in_OG:\n",
    "        species = []\n",
    "        for name in names_species:\n",
    "            species.append(list_proteins.count(name)) # we just need the counts\n",
    "\n",
    "        for pairwise_comb in list(itertools.combinations([i for i in range(len(species))], 2)): # so can deal with more than 2 species\n",
    "            total_n2m_ortholog_count += species[pairwise_comb[0]] * species[pairwise_comb[1]]\n",
    "\n",
    "    return(total_n2m_ortholog_count)\n",
    "\n",
    "def get_total_count_121_orthologs(prot_names_and_group_train):\n",
    "    list_all_OGs = list(set([protein[4] for protein in prot_names_and_group_train]))\n",
    "    \n",
    "    list_proteins_in_OG = [[] for i in range(len(list_all_OGs))]\n",
    "    \n",
    "    for protein in prot_names_and_group_train:\n",
    "        list_proteins_in_OG[list_all_OGs.index(protein[4])].append(protein[0])\n",
    "\n",
    "    names_species = list(set([protein[0] for protein in prot_names_and_group_train]))\n",
    "    print(names_species)\n",
    "    \n",
    "    total_121_ortholog_count = 0\n",
    "    total_121_ortholog_count_groups = 0\n",
    "    for list_proteins in list_proteins_in_OG:\n",
    "\n",
    "        if (len(list(set(list_proteins))) == len(list_proteins) == len(names_species)): # if getting exactly one protein from each species\n",
    "            total_121_ortholog_count_groups += 1\n",
    "            species = []\n",
    "            for name in names_species:\n",
    "                species.append(list_proteins.count(name))\n",
    "    \n",
    "            for pairwise_comb in list(itertools.combinations([i for i in range(len(species))], 2)): # so can deal with more than 2 species\n",
    "                total_121_ortholog_count += species[pairwise_comb[0]] * species[pairwise_comb[1]]\n",
    "\n",
    "    return(total_121_ortholog_count)\n",
    "\n",
    "print(get_total_count_n2m_orthologs(prot_names_and_group_train))\n",
    "print(get_total_count_121_orthologs(prot_names_and_group_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d60418e-4685-427d-adb5-6e65cd79667b",
   "metadata": {},
   "source": [
    "### Working on saved run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "52746990-7d9a-4db4-92d2-ae33bbf5317d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_pca_components: 20\n",
      "n_clusters: 5131\n",
      "Index: 0\n",
      "[1960, 3480]\n",
      "56.32183908045977\n",
      "Index: 1\n",
      "[1729, 1990]\n",
      "86.88442211055276\n",
      "Index: 2\n",
      "[1316, 1587]\n",
      "82.92375551354758\n",
      "Index: 3\n",
      "[1784, 2150]\n",
      "82.97674418604652\n",
      "Elapsed time: 38.9793267250061 seconds\n",
      "{'n_pca20': {'n_clusters5131': {'Naive pairing': [1960, 3480, []], 'Distance-based pairing': [1729, 1990, []], '1-to-1 orthologs': [1316, 1587, []], 'Family Completeness': 0.7231803566208711, 'Adjusted Mutual Information': 0.43927034395026243, '% exact group match': 34.82751900214383}}}\n"
     ]
    }
   ],
   "source": [
    "performance_stored = {}\n",
    "\n",
    "n_samples = Xs_train.shape[0]\n",
    "list_all_groups_no_set = [prot[4] for prot in prot_names_and_group_train]\n",
    "n_species = len(list(set([prot[0] for prot in prot_names_and_group_train])))\n",
    "\n",
    "i_pca = 0\n",
    "for num_pca_components in list_num_pca_components:\n",
    "    i_clu = 0\n",
    "    print(\"num_pca_components: \" + str(num_pca_components))\n",
    "    t0 = time.time()\n",
    "    # Need to recreate training data as it is too heavy to be stored\n",
    "    if num_pca_components==0:\n",
    "        Xs_train_pca = Xs_train\n",
    "    else:\n",
    "        pca = PCA(n_components = num_pca_components, svd_solver = \"full\")\n",
    "        Xs_train_pca = pca.fit_transform(Xs_train)\n",
    "\n",
    "    performance_stored[\"n_pca\" + str(num_pca_components)] = {}\n",
    "\n",
    "    for n_clusters in list_n_clusters:\n",
    "        print(\"n_clusters: \" + str(n_clusters))\n",
    "        performance_stored[\"n_pca\" + str(num_pca_components)][\"n_clusters\" + str(n_clusters)] = {}\n",
    "\n",
    "        # Pair level\n",
    "        results = measure_pairwise_performance(kmeans_saving[\"n_pca\" + str(num_pca_components)][\"n_clusters\" + str(n_clusters)], Xs_train_pca)\n",
    "        Xs_train_pca\n",
    "        for index in [0,1,2,3]:\n",
    "            n_retrieved = 0\n",
    "            n_correct = 0\n",
    "            FalsePositive_pairs_list = []\n",
    "            for prots in results[index]:\n",
    "                if len(list(set([p[4] for p in prots]))) == 1: # verify that they all come from the same group\n",
    "                    if index==2: # for 1:1 orthologs, Additional check that we are not from a n:m configuration\n",
    "                        if list_all_groups_no_set.count(prots[0][4]) == n_species:\n",
    "                            n_correct += 1\n",
    "                        else: # Study False Positives\n",
    "                            FalsePositive_pairs_list.append([prots[0][1], prots[0][4], prots[1][1], prots[1][4]])\n",
    "                    else:\n",
    "                        n_correct += 1\n",
    "                else: # Study False Positives\n",
    "                    FalsePositive_pairs_list.append([prots[0][1], prots[0][4], prots[1][1], prots[1][4]])\n",
    "                n_retrieved += 1\n",
    "            print(\"Index: \" + str(index))\n",
    "            print([n_correct, n_retrieved])\n",
    "\n",
    "            if True: # if not needed to store the false positives\n",
    "                FalsePositive_pairs_list = []\n",
    "\n",
    "            if n_retrieved != 0:\n",
    "                print((n_correct / n_retrieved * 100))\n",
    "            if index==0:\n",
    "                performance_stored[\"n_pca\" + str(num_pca_components)][\"n_clusters\" + str(n_clusters)][\"Naive pairing\"] = [n_correct, n_retrieved, FalsePositive_pairs_list]\n",
    "            if index==1:\n",
    "                performance_stored[\"n_pca\" + str(num_pca_components)][\"n_clusters\" + str(n_clusters)][\"Distance-based pairing\"] = [n_correct, n_retrieved, FalsePositive_pairs_list]\n",
    "            if index==2:\n",
    "                performance_stored[\"n_pca\" + str(num_pca_components)][\"n_clusters\" + str(n_clusters)][\"1-to-1 orthologs\"] = [n_correct, n_retrieved, FalsePositive_pairs_list]\n",
    "\n",
    "        # Group level\n",
    "        results_group = measure_group_performance(kmeans_saving[\"n_pca\" + str(num_pca_components)][\"n_clusters\" + str(n_clusters)])\n",
    "        performance_stored[\"n_pca\" + str(num_pca_components)][\"n_clusters\" + str(n_clusters)][\"Family Completeness\"] = results_group[0]\n",
    "        performance_stored[\"n_pca\" + str(num_pca_components)][\"n_clusters\" + str(n_clusters)][\"Adjusted Mutual Information\"] = results_group[1]\n",
    "        performance_stored[\"n_pca\" + str(num_pca_components)][\"n_clusters\" + str(n_clusters)][\"% exact group match\"] = results_group[2] * 100\n",
    "\n",
    "        i_clu+=1\n",
    "    i_pca+=1\n",
    "    print(\"Elapsed time: \" + str(time.time() - t0) + \" seconds\")\n",
    "\n",
    "print(performance_stored)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a863da-068f-457a-be25-a0f3f5bce943",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
